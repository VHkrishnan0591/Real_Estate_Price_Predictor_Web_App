{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\harik\\\\OneDrive\\\\Desktop\\\\HARIKRISHNAN_DETAILS\\\\Real_Estate_Predictor_Web_App\\\\Real_Estate_Price_Predictor_Web_App\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\harik\\\\OneDrive\\\\Desktop\\\\HARIKRISHNAN_DETAILS\\\\Real_Estate_Predictor_Web_App\\\\Real_Estate_Price_Predictor_Web_App'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    X_train_scaled_data_file: Path\n",
    "    X_test_scaled_data_file: Path\n",
    "    Y_train_data_file: Path\n",
    "    Y_test_data_file: Path\n",
    "    performance_metrics_file_path: Path\n",
    "    params_list_of_models:list\n",
    "    params_polynomial_type:str\n",
    "    params_polynomial_model:str\n",
    "    params_polynomial_degree:int\n",
    "    params_kernel:str\n",
    "    params_C:int\n",
    "    params_n_estimators:int\n",
    "    params_max_depth:int\n",
    "    params_number_of_iteration: int\n",
    "    params_cv: int\n",
    "    params_verbose: int\n",
    "    params_random_state_for_randomised_cv: int\n",
    "    params_n_jobs: int\n",
    "    params_list_of_models_for_hyper_parameter_tuning: list\n",
    "    params_ridge_regression_solver: list\n",
    "    params_tol: list\n",
    "    params_SVR_gamma: list\n",
    "    params_max_features: list\n",
    "    params_hyper_n_estimators: list\n",
    "    params_gradient_boost_learning_rate: list\n",
    "    params_subsample: list\n",
    "    params_xgboost_learning_rate: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_estate_price_predictor.constants import *\n",
    "from real_estate_price_predictor.utils.common import read_yaml, create_directories,save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_training(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config= ModelTrainingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            X_train_scaled_data_file = config.X_train_scaled_data_file,\n",
    "            X_test_scaled_data_file = config.X_test_scaled_data_file,\n",
    "            Y_train_data_file = config.Y_train_data_file,\n",
    "            Y_test_data_file = config.Y_test_data_file,\n",
    "            performance_metrics_file_path = config.performance_metrics_file_path,\n",
    "            params_list_of_models = self.params.list_of_models,\n",
    "            params_polynomial_type = self.params.polynomial_type,\n",
    "            params_polynomial_model = self.params.polynomial_model,\n",
    "            params_polynomial_degree = self.params.polynomial_degree,\n",
    "            params_kernel = self.params.kernel,\n",
    "            params_C = self.params.C,\n",
    "            params_n_estimators = self.params.n_estimators,\n",
    "            params_max_depth = self.params.max_depth,\n",
    "            params_number_of_iteration = self.params.number_of_iteration,\n",
    "            params_cv = self.params.cv,\n",
    "            params_verbose = self.params.verbose,\n",
    "            params_random_state_for_randomised_cv = self.params.random_state_for_randomised_cv,\n",
    "            params_n_jobs = self.params.n_jobs,\n",
    "            params_list_of_models_for_hyper_parameter_tuning = self.params.list_of_models_for_hyper_parameter_tuning,\n",
    "            params_ridge_regression_solver = self.params.ridge_regression_solver,\n",
    "            params_tol = self.params.tol,\n",
    "            params_SVR_gamma = self.params.SVR_gamma,\n",
    "            params_max_features = self.params.max_features,\n",
    "            params_hyper_n_estimators = self.params.hyper_n_estimators,\n",
    "            params_gradient_boost_learning_rate = self.params.gradient_boost_learning_rate,\n",
    "            params_subsample = self.params.subsample,\n",
    "            params_xgboost_learning_rate = self.params.xgboost_learning_rate\n",
    "\n",
    "        )\n",
    "\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self,config=ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.best_params ={}\n",
    "    \n",
    "    def model_training(self):\n",
    "        X_test_data = pd.read_csv(self.config.X_test_scaled_data_file)\n",
    "        X_train_data  = pd.read_csv(self.config.X_train_scaled_data_file)\n",
    "        y_train = pd.read_csv(self.config.Y_train_data_file)\n",
    "        y_test = pd.read_csv(self.config.Y_test_data_file)\n",
    "        list_of_models = self.config.params_list_of_models\n",
    "        r2_score_of_models=[]\n",
    "        adjusted_r2_score =[]\n",
    "        mse=[]\n",
    "        for i  in list_of_models:\n",
    "            if i == 'Linear Regression':\n",
    "                model = LinearRegression()\n",
    "            elif i == 'Ridge Regression':\n",
    "                model = Ridge()\n",
    "            elif i == 'Polynomial Regression':\n",
    "                model = Pipeline([(self.config.params_polynomial_type, PolynomialFeatures(degree=self.config.params_polynomial_degree)),(self.config.params_polynomial_model, LinearRegression())])\n",
    "            elif i == 'SVR':\n",
    "                model = SVR(kernel=self.config.params_kernel, C=self.config.params_C)\n",
    "            elif i == 'Random Forrest Regressor':\n",
    "                model = RandomForestRegressor(n_estimators=self.config.params_n_estimators)\n",
    "            elif i == 'AdaBoost Regressor':\n",
    "                model = AdaBoostRegressor()\n",
    "            elif i == 'Gradient Boosting Regressor':\n",
    "                model = GradientBoostingRegressor()\n",
    "            elif i == 'XGBRegressor':\n",
    "                model = XGBRegressor()\n",
    "            else:\n",
    "                model = DecisionTreeRegressor(max_depth=self.config.params_max_depth)\n",
    "        # Train the model on the training data\n",
    "\n",
    "            model.fit(X_train_data, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "\n",
    "            y_pred = model.predict(X_test_data)\n",
    "\n",
    "        # Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "            \n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            r2_score_of_models.append(r2)\n",
    "\n",
    "        # Calculate the adjusted R²\n",
    "\n",
    "            n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "            p = X_test_data.shape[1]  # Number of features in the model\n",
    "            adjusted_r2_score.append(1 - (1 - r2) * (n - 1) / (n - p - 1))\n",
    "            mse.append(mean_squared_error(y_test, y_pred))\n",
    "        data = {'Models': list_of_models, 'Adjusted_R2_Score': adjusted_r2_score, 'R2_Score': r2_score_of_models , 'Mean_Squared_Error': mse}\n",
    "        performance_metrics = pd.DataFrame.from_dict(data)\n",
    "        performance_metrics.set_index('Models', inplace = False)\n",
    "        return performance_metrics\n",
    "    \n",
    "    def hyperparameter_tuning(self,performance_metrics:pd.DataFrame):\n",
    "        X_test_data = pd.read_csv(self.config.X_test_scaled_data_file)\n",
    "        X_train_data  = pd.read_csv(self.config.X_train_scaled_data_file)\n",
    "        y_train = pd.read_csv(self.config.Y_train_data_file)\n",
    "        y_test = pd.read_csv(self.config.Y_test_data_file)\n",
    "        list_of_models_for_hyper_parameter_tuning = self.config.params_list_of_models_for_hyper_parameter_tuning\n",
    "        r2_score_of_models_hyper=[]\n",
    "        adjusted_r2_score_hyper =[]\n",
    "        mse_hyper = []\n",
    "        for i in list_of_models_for_hyper_parameter_tuning:\n",
    "            if i == 'Hyper Parameter Ridge Regression':\n",
    "                random_grid = {'alpha': [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)],\n",
    "                            'solver': self.config.params_ridge_regression_solver,\n",
    "                            'tol': self.config.params_tol}\n",
    "                model = Ridge()\n",
    "                rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=self.config.params_number_of_iteration,cv=self.config.params_cv,verbose=self.config.params_verbose,\n",
    "                                            random_state=self.config.params_random_state_for_randomised_cv,n_jobs=self.config.params_n_jobs)\n",
    "            elif i == 'Hyper Parameter Support Vector Regression':\n",
    "                random_grid = {'kernel': [self.config.params_kernel],\n",
    "                            'C': [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)] + [int(x) for x in np.arange(1, 11)],\n",
    "                            'epsilon': [float(x) for x in np.linspace(start = 0.01, stop = 0.1, num = 10)] + [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)],\n",
    "                            'gamma': self.config.params_SVR_gamma\n",
    "                            }\n",
    "                model = SVR()\n",
    "                rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=self.config.params_number_of_iteration,cv=self.config.params_cv,verbose=self.config.params_verbose,\n",
    "                                            random_state=self.config.params_random_state_for_randomised_cv,n_jobs=self.config.params_n_jobs)\n",
    "            elif i == 'Hyper Parameter Randomn Forrest Regression':\n",
    "                random_grid = {'max_features':self.config.params_max_features,\n",
    "                    'n_estimators': self.config.params_hyper_n_estimators\n",
    "                    }\n",
    "                model = RandomForestRegressor()\n",
    "                rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=self.config.params_number_of_iteration,cv=self.config.params_cv,verbose=self.config.params_verbose,\n",
    "                                            random_state=self.config.params_random_state_for_randomised_cv,n_jobs=self.config.params_n_jobs)\n",
    "            elif i == 'Hyper Parameter Gradient Boost Regression':\n",
    "                random_grid = {\n",
    "                            'learning_rate':self.config.params_gradient_boost_learning_rate,\n",
    "                            'subsample':self.config.params_subsample,\n",
    "                            'n_estimators': self.config.params_hyper_n_estimators\n",
    "                    }\n",
    "                model = GradientBoostingRegressor()\n",
    "                rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=self.config.params_number_of_iteration,cv=self.config.params_cv,verbose=self.config.params_verbose,\n",
    "                                            random_state=self.config.params_random_state_for_randomised_cv,n_jobs=self.config.params_n_jobs)\n",
    "            elif i == 'Hyper Parameter XGBoost Regression':\n",
    "                random_grid = {\n",
    "                            'learning_rate':self.config.params_xgboost_learning_rate,\n",
    "                            'n_estimators': self.config.params_hyper_n_estimators\n",
    "                    }\n",
    "                model = XGBRegressor()\n",
    "                rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=self.config.params_number_of_iteration,cv=self.config.params_cv,verbose=self.config.params_verbose,\n",
    "                                            random_state=self.config.params_random_state_for_randomised_cv,n_jobs=self.config.params_n_jobs)\n",
    "            rf_randomcv.fit(X_train_data,y_train)\n",
    "            best_random_grid=rf_randomcv.best_estimator_\n",
    "            y_pred=best_random_grid.predict(X_test_data)\n",
    "            self.best_params[i] = rf_randomcv.best_estimator_\n",
    "            # Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "            r2_hyper = r2_score(y_test, y_pred)\n",
    "            r2_score_of_models_hyper.append(r2_hyper)\n",
    "\n",
    "            # Calculate the adjusted R²\n",
    "\n",
    "            n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "            p = X_test_data.shape[1]  # Number of features in the model\n",
    "            adjusted_r2_hyper = 1 - (1 - r2_hyper) * (n - 1) / (n - p - 1)\n",
    "            adjusted_r2_score_hyper.append(adjusted_r2_hyper)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse_hyper.append(mse)\n",
    "        data = {'Models': list_of_models_for_hyper_parameter_tuning, 'Adjusted_R2_Score': adjusted_r2_score_hyper, 'R2_Score': r2_score_of_models_hyper , 'Mean_Squared_Error': mse_hyper}\n",
    "        performance_metrics_hyper = pd.DataFrame.from_dict(data)\n",
    "        performance_metrics_hyper.set_index('Models', inplace = False)\n",
    "        combined_results = pd.concat([performance_metrics,performance_metrics_hyper],axis=0)\n",
    "        combined_results = combined_results.sort_values(['Adjusted_R2_Score'],ascending=False)\n",
    "        return combined_results\n",
    "    \n",
    "    def save_the_best_model(self,combined_performance_metrics:pd.DataFrame):\n",
    "        best_model_name = combined_performance_metrics.sort_values(['Adjusted_R2_Score'],ascending=False).head(1)['Models'].values[0]\n",
    "        if best_model_name == 'Hyper Parameter Ridge Regression':\n",
    "            model = self.best_params[best_model_name]\n",
    "        elif best_model_name == 'Hyper Parameter Support Vector Regression':\n",
    "            model = self.best_params[best_model_name]\n",
    "        elif best_model_name == 'Hyper Parameter Randomn Forrest Regression':\n",
    "            model = self.best_params[best_model_name]\n",
    "        elif best_model_name == 'Hyper Parameter Gradient Boost Regression':\n",
    "            model = self.best_params[best_model_name]\n",
    "        elif best_model_name == 'Hyper Parameter XGBoost Regression':\n",
    "            model = self.best_params[best_model_name]\n",
    "        elif best_model_name == 'linear Regression':\n",
    "            model = LinearRegression()\n",
    "        elif best_model_name == 'Ridge Regression':\n",
    "            model = Ridge()\n",
    "        elif best_model_name == 'Polynomial Regression':\n",
    "            model = Pipeline([(self.config.params_polynomial_type, PolynomialFeatures(degree=self.config.params_polynomial_degree)),(self.config.params_polynomial_model, LinearRegression())])\n",
    "        elif best_model_name == 'SVR':\n",
    "            model = SVR(kernel=self.config.params_kernel, C=self.config.params_C)\n",
    "        elif best_model_name == 'Random Forrest Regressor':\n",
    "            model = RandomForestRegressor(n_estimators=self.config.params_n_estimators)\n",
    "        elif best_model_name == 'AdaBoost Regressor':\n",
    "            model = AdaBoostRegressor()\n",
    "        elif best_model_name == 'Gradient Boosting Regressor':\n",
    "            model = GradientBoostingRegressor()\n",
    "        elif best_model_name == 'XGBRegressor':\n",
    "            model = XGBRegressor()\n",
    "        else:\n",
    "            model = DecisionTreeRegressor(max_depth=self.config.params_max_depth)\n",
    "\n",
    "        combined_performance_metrics.to_csv(self.config.performance_metrics_file_path)\n",
    "        best_model_name = best_model_name.replace(\" \",\"\") + '.pkl'\n",
    "        file_path = os.path.join(self.config.root_dir,best_model_name)\n",
    "        save_object(file_path,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 20:19:13,887: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-03 20:19:13,901: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-03 20:19:13,905: INFO: common: created directory at: artifacts]\n",
      "[2024-10-03 20:19:13,907: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 18 is smaller than n_iter=50. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 54.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "13 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.85528761 0.86816364 0.87918457 0.88129381 0.88302116 0.8840269\n",
      " 0.84901969 0.87326545 0.87711775 0.88322735 0.88476406 0.88558035\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 24 is smaller than n_iter=50. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training()\n",
    "    model_training = ModelTraining(config=model_training_config)\n",
    "    results = model_training.model_training()\n",
    "    combined_performance_metrics = model_training.hyperparameter_tuning(results)\n",
    "    model_training.save_the_best_model(combined_performance_metrics)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Adjusted_R2_Score</th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>Mean_Squared_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyper Parameter Randomn Forrest Regression</td>\n",
       "      <td>0.879479</td>\n",
       "      <td>0.898596</td>\n",
       "      <td>0.013885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyper Parameter Support Vector Regression</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.894597</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forrest Regressor</td>\n",
       "      <td>0.873196</td>\n",
       "      <td>0.893310</td>\n",
       "      <td>0.014609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.871083</td>\n",
       "      <td>0.891532</td>\n",
       "      <td>0.014853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.868095</td>\n",
       "      <td>0.889018</td>\n",
       "      <td>0.015197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hyper Parameter Ridge Regression</td>\n",
       "      <td>0.868095</td>\n",
       "      <td>0.889018</td>\n",
       "      <td>0.015197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>0.864571</td>\n",
       "      <td>0.886053</td>\n",
       "      <td>0.015603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>0.880598</td>\n",
       "      <td>0.016350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyper Parameter Gradient Boost Regression</td>\n",
       "      <td>0.854211</td>\n",
       "      <td>0.877336</td>\n",
       "      <td>0.016797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyper Parameter XGBoost Regression</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.839446</td>\n",
       "      <td>0.864913</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression</td>\n",
       "      <td>0.773495</td>\n",
       "      <td>0.809423</td>\n",
       "      <td>0.026096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>0.738599</td>\n",
       "      <td>0.780062</td>\n",
       "      <td>0.030117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>0.702408</td>\n",
       "      <td>0.749612</td>\n",
       "      <td>0.034286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Models  Adjusted_R2_Score  R2_Score  \\\n",
       "2  Hyper Parameter Randomn Forrest Regression           0.879479  0.898596   \n",
       "1   Hyper Parameter Support Vector Regression           0.874726  0.894597   \n",
       "4                    Random Forrest Regressor           0.873196  0.893310   \n",
       "0                           Linear Regression           0.871083  0.891532   \n",
       "1                            Ridge Regression           0.868095  0.889018   \n",
       "0            Hyper Parameter Ridge Regression           0.868095  0.889018   \n",
       "7                 Gradient Boosting Regressor           0.864571  0.886053   \n",
       "3                                         SVR           0.858088  0.880598   \n",
       "3   Hyper Parameter Gradient Boost Regression           0.854211  0.877336   \n",
       "4          Hyper Parameter XGBoost Regression           0.846933  0.871213   \n",
       "8                                XGBRegressor           0.839446  0.864913   \n",
       "2                       Polynomial Regression           0.773495  0.809423   \n",
       "6                          AdaBoost Regressor           0.738599  0.780062   \n",
       "5                     Decision Tree Regressor           0.702408  0.749612   \n",
       "\n",
       "   Mean_Squared_Error  \n",
       "2            0.013885  \n",
       "1            0.014433  \n",
       "4            0.014609  \n",
       "0            0.014853  \n",
       "1            0.015197  \n",
       "0            0.015197  \n",
       "7            0.015603  \n",
       "3            0.016350  \n",
       "3            0.016797  \n",
       "4            0.017635  \n",
       "8            0.018498  \n",
       "2            0.026096  \n",
       "6            0.030117  \n",
       "5            0.034286  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hyper Parameter Ridge Regression': Ridge(alpha=1, solver='lsqr', tol=1e-06),\n",
       " 'Hyper Parameter Support Vector Regression': SVR(C=0.6, epsilon=0.01),\n",
       " 'Hyper Parameter Randomn Forrest Regression': RandomForestRegressor(max_features='sqrt', n_estimators=256),\n",
       " 'Hyper Parameter Gradient Boost Regression': GradientBoostingRegressor(learning_rate=0.05, n_estimators=256, subsample=0.6),\n",
       " 'Hyper Parameter XGBoost Regression': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=128, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
      "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n"
     ]
    }
   ],
   "source": [
    "x = combined_performance_metrics[1]['Hyper Parameter Gradient Boost Regression']\n",
    "y = GradientBoostingRegressor(learning_rate=0.05, n_estimators=256, subsample=0.6)\n",
    "print(type(x))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hyper Parameter Randomn Forrest Regression'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_name = combined_performance_metrics[0].sort_values(['Adjusted_R2_Score'],ascending=False).head(1)['Models'].values[0]\n",
    "best_model_name\n",
    "if best_model_name == 'Hyper Parameter Ridge Regression':\n",
    "    model = combined_performance_metrics[1][best_model_name]\n",
    "elif best_model_name == 'Hyper Parameter Support Vector Regression':\n",
    "    model = combined_performance_metrics[1][best_model_name]\n",
    "elif best_model_name == 'Hyper Parameter Randomn Forrest Regression':\n",
    "    model = combined_performance_metrics[1][best_model_name]\n",
    "elif best_model_name == 'Hyper Parameter Gradient Boost Regression':\n",
    "    model = combined_performance_metrics[1][best_model_name]\n",
    "elif best_model_name == 'Hyper Parameter XGBoost Regression':\n",
    "    model = combined_performance_metrics[1][best_model_name]\n",
    "elif best_model_name == 'linear Regression':\n",
    "    model = LinearRegression()\n",
    "elif best_model_name == 'Ridge Regression':\n",
    "    model = Ridge()\n",
    "elif best_model_name == 'Polynomial Regression':\n",
    "    model = Pipeline([(self.config.params_polynomial_type, PolynomialFeatures(degree=self.config.params_polynomial_degree)),(self.config.params_polynomial_model, LinearRegression())])\n",
    "elif best_model_name == 'SVR':\n",
    "    model = SVR(kernel=self.config.params_kernel, C=self.config.params_C)\n",
    "elif best_model_name == 'Random Forrest Regressor':\n",
    "    model = RandomForestRegressor(n_estimators=self.config.params_n_estimators)\n",
    "elif best_model_name == 'AdaBoost Regressor':\n",
    "    model = AdaBoostRegressor()\n",
    "elif best_model_name == 'Gradient Boosting Regressor':\n",
    "    model = GradientBoostingRegressor()\n",
    "elif best_model_name == 'XGBRegressor':\n",
    "    model = XGBRegressor()\n",
    "else:\n",
    "    model = DecisionTreeRegressor(max_depth=self.config.params_max_depth)\n",
    "\n",
    "combined_performance_metrics[0].to_csv(self.config.performance_metrics_file_path)\n",
    "best_model_name = best_model_name.replace(\" \",\"\") + '.pkl'\n",
    "file_path = os.path.join(self.config.root_dir,best_model_name)\n",
    "save_object(file_path,model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Adjusted_R2_Score</th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>Mean_Squared_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.871083</td>\n",
       "      <td>0.891532</td>\n",
       "      <td>0.014853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.868095</td>\n",
       "      <td>0.889018</td>\n",
       "      <td>0.015197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression</td>\n",
       "      <td>0.773495</td>\n",
       "      <td>0.809423</td>\n",
       "      <td>0.026096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>0.880598</td>\n",
       "      <td>0.016350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forrest Regressor</td>\n",
       "      <td>0.874312</td>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.014481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>0.702408</td>\n",
       "      <td>0.749612</td>\n",
       "      <td>0.034286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>0.753011</td>\n",
       "      <td>0.792189</td>\n",
       "      <td>0.028456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>0.860381</td>\n",
       "      <td>0.882528</td>\n",
       "      <td>0.016086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.839446</td>\n",
       "      <td>0.864913</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Models  Adjusted_R2_Score  R2_Score  \\\n",
       "0            Linear Regression           0.871083  0.891532   \n",
       "1             Ridge Regression           0.868095  0.889018   \n",
       "2        Polynomial Regression           0.773495  0.809423   \n",
       "3                          SVR           0.858088  0.880598   \n",
       "4     Random Forrest Regressor           0.874312  0.894249   \n",
       "5      Decision Tree Regressor           0.702408  0.749612   \n",
       "6           AdaBoost Regressor           0.753011  0.792189   \n",
       "7  Gradient Boosting Regressor           0.860381  0.882528   \n",
       "8                 XGBRegressor           0.839446  0.864913   \n",
       "\n",
       "   Mean_Squared_Error  \n",
       "0            0.014853  \n",
       "1            0.015197  \n",
       "2            0.026096  \n",
       "3            0.016350  \n",
       "4            0.014481  \n",
       "5            0.034286  \n",
       "6            0.028456  \n",
       "7            0.016086  \n",
       "8            0.018498  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = pd.read_csv(r'C:\\Users\\harik\\OneDrive\\Desktop\\HARIKRISHNAN_DETAILS\\Real_Estate_Predictor_Web_App\\Real_Estate_Price_Predictor_Web_App\\artifacts\\train_test_data_scaled\\X_test_scaled.csv')\n",
    "X_train_data  = pd.read_csv(r'C:\\Users\\harik\\OneDrive\\Desktop\\HARIKRISHNAN_DETAILS\\Real_Estate_Predictor_Web_App\\Real_Estate_Price_Predictor_Web_App\\artifacts\\train_test_data_scaled\\X_train_scaled.csv')\n",
    "y_train = pd.read_csv(r'C:\\Users\\harik\\OneDrive\\Desktop\\HARIKRISHNAN_DETAILS\\Real_Estate_Predictor_Web_App\\Real_Estate_Price_Predictor_Web_App\\artifacts\\train_test_data\\Y_train.csv')\n",
    "y_test = pd.read_csv(r'C:\\Users\\harik\\OneDrive\\Desktop\\HARIKRISHNAN_DETAILS\\Real_Estate_Predictor_Web_App\\Real_Estate_Price_Predictor_Web_App\\artifacts\\train_test_data\\Y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 24 is smaller than n_iter=50. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 18 is smaller than n_iter=50. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 24 is smaller than n_iter=50. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Adjusted_R2_Score</th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>Mean_Squared_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hyper Parameter Ridge Regression</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyper Parameter Support Vector Regression</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.894597</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyper Parameter Randomn Forrest Regression</td>\n",
       "      <td>0.880867</td>\n",
       "      <td>0.899764</td>\n",
       "      <td>0.013726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyper Parameter Gradient Boost Regression</td>\n",
       "      <td>0.861119</td>\n",
       "      <td>0.883148</td>\n",
       "      <td>0.016001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyper Parameter XGBoost Regression</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Models  Adjusted_R2_Score  R2_Score  \\\n",
       "0            Hyper Parameter Ridge Regression           0.846933  0.871213   \n",
       "1   Hyper Parameter Support Vector Regression           0.874726  0.894597   \n",
       "2  Hyper Parameter Randomn Forrest Regression           0.880867  0.899764   \n",
       "3   Hyper Parameter Gradient Boost Regression           0.861119  0.883148   \n",
       "4          Hyper Parameter XGBoost Regression           0.846933  0.871213   \n",
       "\n",
       "   Mean_Squared_Error  \n",
       "0            0.017635  \n",
       "1            0.014433  \n",
       "2            0.013726  \n",
       "3            0.016001  \n",
       "4            0.017635  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "list_of_models_for_hyper_parameter_tuning = ['Hyper Parameter Ridge Regression', 'Hyper Parameter Support Vector Regression', 'Hyper Parameter Randomn Forrest Regression', 'Hyper Parameter Gradient Boost Regression','Hyper Parameter XGBoost Regression']\n",
    "best_params ={}\n",
    "r2_score_of_models_hyper=[]\n",
    "adjusted_r2_score_hyper =[]\n",
    "mse_hyper = []\n",
    "for i in list_of_models_for_hyper_parameter_tuning:\n",
    "    if i == 'Ridge Regression':\n",
    "        random_grid = {'alpha': [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)],\n",
    "                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "                    'tol': [1e-4, 1e-5, 1e-6]}\n",
    "        model = Ridge()\n",
    "        rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n",
    "                                    random_state=100,n_jobs=-1)\n",
    "    elif i == 'Hyper Parameter Support Vector Regression':\n",
    "        random_grid = {'kernel': ['rbf'],\n",
    "                    'C': [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)] + [int(x) for x in np.arange(1, 11)],\n",
    "                    'epsilon': [float(x) for x in np.linspace(start = 0.01, stop = 0.1, num = 10)] + [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)],\n",
    "                    'gamma': ['scale','auto']\n",
    "                    }\n",
    "        model = SVR()\n",
    "        rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                                    random_state=100,n_jobs=-1)\n",
    "    elif i == 'Hyper Parameter Randomn Forrest Regression':\n",
    "        random_grid = {'max_features':['sqrt','log2',None],\n",
    "               'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "        model = RandomForestRegressor()\n",
    "        rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                                    random_state=100,n_jobs=-1)\n",
    "    elif i == 'Hyper Parameter Gradient Boost Regression':\n",
    "        random_grid = {\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "        model = GradientBoostingRegressor()\n",
    "        rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                                    random_state=100,n_jobs=-1)\n",
    "    elif i == 'Hyper Parameter XGBoost Regression':\n",
    "        random_grid = {\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "        model = XGBRegressor()\n",
    "        rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                                    random_state=100,n_jobs=-1)\n",
    "    rf_randomcv.fit(X_train_data,y_train)\n",
    "    best_random_grid=rf_randomcv.best_estimator_\n",
    "    y_pred=best_random_grid.predict(X_test_data)\n",
    "    best_params[i] = rf_randomcv.best_estimator_\n",
    "    # Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "    r2_hyper = r2_score(y_test, y_pred)\n",
    "    r2_score_of_models_hyper.append(r2_hyper)\n",
    "\n",
    "    # Calculate the adjusted R²\n",
    "\n",
    "    n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "    p = X_test_data.shape[1]  # Number of features in the model\n",
    "    adjusted_r2_hyper = 1 - (1 - r2_hyper) * (n - 1) / (n - p - 1)\n",
    "    adjusted_r2_score_hyper.append(adjusted_r2_hyper)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_hyper.append(mse)\n",
    "data = {'Models': list_of_models_for_hyper_parameter_tuning, 'Adjusted_R2_Score': adjusted_r2_score_hyper, 'R2_Score': r2_score_of_models_hyper , 'Mean_Squared_Error': mse_hyper}\n",
    "performance_metrics_hyper = pd.DataFrame.from_dict(data)\n",
    "performance_metrics_hyper.set_index('Models', inplace = False)\n",
    "combined_results = pd.concat([results,performance_metrics_hyper],axis=0)\n",
    "combined_results = combined_results.sort_values(['Adjusted_R2_Score'],ascending=False)\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Adjusted_R2_Score</th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>Mean_Squared_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyper Parameter Randomn Forrest Regression</td>\n",
       "      <td>0.880867</td>\n",
       "      <td>0.899764</td>\n",
       "      <td>0.013726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyper Parameter Support Vector Regression</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.894597</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forrest Regressor</td>\n",
       "      <td>0.874312</td>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.014481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.871083</td>\n",
       "      <td>0.891532</td>\n",
       "      <td>0.014853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.868095</td>\n",
       "      <td>0.889018</td>\n",
       "      <td>0.015197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyper Parameter Gradient Boost Regression</td>\n",
       "      <td>0.861119</td>\n",
       "      <td>0.883148</td>\n",
       "      <td>0.016001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>0.860381</td>\n",
       "      <td>0.882528</td>\n",
       "      <td>0.016086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>0.880598</td>\n",
       "      <td>0.016350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hyper Parameter Ridge Regression</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyper Parameter XGBoost Regression</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.017635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.839446</td>\n",
       "      <td>0.864913</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Regression</td>\n",
       "      <td>0.773495</td>\n",
       "      <td>0.809423</td>\n",
       "      <td>0.026096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost Regressor</td>\n",
       "      <td>0.753011</td>\n",
       "      <td>0.792189</td>\n",
       "      <td>0.028456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>0.702408</td>\n",
       "      <td>0.749612</td>\n",
       "      <td>0.034286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Models  Adjusted_R2_Score  R2_Score  \\\n",
       "2  Hyper Parameter Randomn Forrest Regression           0.880867  0.899764   \n",
       "1   Hyper Parameter Support Vector Regression           0.874726  0.894597   \n",
       "4                    Random Forrest Regressor           0.874312  0.894249   \n",
       "0                           Linear Regression           0.871083  0.891532   \n",
       "1                            Ridge Regression           0.868095  0.889018   \n",
       "3   Hyper Parameter Gradient Boost Regression           0.861119  0.883148   \n",
       "7                 Gradient Boosting Regressor           0.860381  0.882528   \n",
       "3                                         SVR           0.858088  0.880598   \n",
       "0            Hyper Parameter Ridge Regression           0.846933  0.871213   \n",
       "4          Hyper Parameter XGBoost Regression           0.846933  0.871213   \n",
       "8                                XGBRegressor           0.839446  0.864913   \n",
       "2                       Polynomial Regression           0.773495  0.809423   \n",
       "6                          AdaBoost Regressor           0.753011  0.792189   \n",
       "5                     Decision Tree Regressor           0.702408  0.749612   \n",
       "\n",
       "   Mean_Squared_Error  \n",
       "2            0.013726  \n",
       "1            0.014433  \n",
       "4            0.014481  \n",
       "0            0.014853  \n",
       "1            0.015197  \n",
       "3            0.016001  \n",
       "7            0.016086  \n",
       "3            0.016350  \n",
       "0            0.017635  \n",
       "4            0.017635  \n",
       "8            0.018498  \n",
       "2            0.026096  \n",
       "6            0.028456  \n",
       "5            0.034286  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results = pd.concat([results,performance_metrics_hyper],axis=0)\n",
    "combined_results = combined_results.sort_values(['Adjusted_R2_Score'],ascending=False)\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hyper Parameter Ridge Regression': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=128, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " 'Hyper Parameter Support Vector Regression': SVR(C=0.6, epsilon=0.01),\n",
       " 'Hyper Parameter Randomn Forrest Regression': RandomForestRegressor(max_features='sqrt', n_estimators=64),\n",
       " 'Hyper Parameter Gradient Boost Regression': GradientBoostingRegressor(learning_rate=0.05, n_estimators=256, subsample=0.6),\n",
       " 'Hyper Parameter XGBoost Regression': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=128, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "The R2 score is  0.8890182488758518\n",
      "The Mean squared error is  0.015196984496418655\n",
      "The adjusted R2 score is  0.868095459729496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "33 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1251, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 918, in fit\n",
      "    raise ValueError(\n",
      "ValueError: 'lbfgs' solver can be used only when positive=True. Please use another solver.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.88450792 0.88112671 0.87775296 0.88282036 0.87775106 0.88932731\n",
      " 0.88932731 0.88112671 0.88617379 0.88282064 0.88112621 0.88779689\n",
      "        nan 0.87943203 0.8777507  0.88450784 0.87943482        nan\n",
      " 0.87607815 0.88450779 0.88617441 0.88617258 0.87943484 0.8777507\n",
      " 0.88932731 0.8777507  0.87607814 0.89061758 0.88779679 0.88779688\n",
      " 0.88282065 0.88112641 0.88282064 0.88932731 0.88112678 0.88112671\n",
      " 0.87775155 0.88282062 0.88617444 0.88282064 0.88282064 0.87607813\n",
      " 0.87607813 0.88112243 0.89061745 0.88450784 0.88779679 0.88932731\n",
      " 0.88932551 0.88282064 0.87943482 0.87775063 0.88779691 0.88112671\n",
      "        nan 0.88779691        nan 0.88779691 0.88112243        nan\n",
      "        nan 0.89061764 0.8777507  0.87607812 0.89061737 0.88932731\n",
      "        nan 0.88450799 0.88617444 0.88932623 0.87607908 0.87943482\n",
      "        nan        nan 0.88282064 0.8861746         nan 0.8777507\n",
      " 0.89061758 0.88932731 0.88112671 0.87607813 0.88932733 0.87943488\n",
      " 0.88112671 0.8777507  0.89061758 0.88112671 0.88617444 0.88450784\n",
      " 0.87943478 0.87775073 0.87607813 0.88617469 0.87775089        nan\n",
      " 0.88282068 0.8777507  0.88450784 0.88779691]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Choosing the best parameter for Ridge Regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Alpha value in the Ridge Regrssion\n",
    "alpha = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "# Solver for the ridge regression\n",
    "solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "# Tolereance the stopping criteria for the Ridge Regression\n",
    "tol = [1e-4, 1e-5, 1e-6]\n",
    "# Create the random grid\n",
    "random_grid = {'alpha': alpha,\n",
    "               'solver': solver,\n",
    "               'tol': tol}\n",
    "model = Ridge()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_data,y_train)\n",
    "best_random_grid=rf_randomcv.best_estimator_\n",
    "y_pred=best_random_grid.predict(X_test_data)\n",
    "# Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the adjusted R²\n",
    "\n",
    "n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "p = X_test_data.shape[1]  # Number of features in the model\n",
    "adjusted_r2_linear = 1 - (1 - r2_linear) * (n - 1) / (n - p - 1)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(\"The R2 score is \",r2_linear)\n",
    "print(\"The Mean squared error is \",mse_linear)\n",
    "print(\"The adjusted R2 score is \",adjusted_r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tol': 1e-06, 'solver': 'lsqr', 'alpha': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "The R2 score is  0.8945971419238966\n",
      "The Mean squared error is  0.014433053937569613\n",
      "The adjusted R2 score is  0.8747261113029918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Choosing the best parameter for Support Vector Regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Kernel for the Support Vector regression\n",
    "kernel = ['rbf']\n",
    "# C Value for the Support Vector regression\n",
    "C = [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)] + [int(x) for x in np.arange(1, 11)]\n",
    "# Epsilon value for the Support Vector regression\n",
    "epsilon = [float(x) for x in np.linspace(start = 0.01, stop = 0.1, num = 10)] + [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)]\n",
    "# Gamma value for the Support Vector regression\n",
    "gamma = ['scale','auto']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'kernel': kernel,\n",
    "               'C': C,\n",
    "               'epsilon': epsilon,\n",
    "               'gamma': gamma\n",
    "              }\n",
    "model = SVR()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_data,y_train)\n",
    "best_random_grid=rf_randomcv.best_estimator_\n",
    "y_pred=best_random_grid.predict(X_test_data)\n",
    "# Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the adjusted R²\n",
    "\n",
    "n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "p = X_test_data.shape[1]  # Number of features in the model\n",
    "adjusted_r2_linear = 1 - (1 - r2_linear) * (n - 1) / (n - p - 1)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(\"The R2 score is \",r2_linear)\n",
    "print(\"The Mean squared error is \",mse_linear)\n",
    "print(\"The adjusted R2 score is \",adjusted_r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf', 'gamma': 'scale', 'epsilon': 0.01, 'C': 0.6}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 18 is smaller than n_iter=50. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is  0.9001948426994395\n",
      "The Mean squared error is  0.013666547993665778\n",
      "The adjusted R2 score is  0.8813791163231043\n"
     ]
    }
   ],
   "source": [
    "random_grid = {'max_features':['sqrt','log2',None],\n",
    "               'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "model = RandomForestRegressor()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_data,y_train)\n",
    "best_random_grid=rf_randomcv.best_estimator_\n",
    "y_pred=best_random_grid.predict(X_test_data)\n",
    "# Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the adjusted R²\n",
    "\n",
    "n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "p = X_test_data.shape[1]  # Number of features in the model\n",
    "adjusted_r2_linear = 1 - (1 - r2_linear) * (n - 1) / (n - p - 1)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(\"The R2 score is \",r2_linear)\n",
    "print(\"The Mean squared error is \",mse_linear)\n",
    "print(\"The adjusted R2 score is \",adjusted_r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 256, 'max_features': 'sqrt'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is  0.8773038118882264\n",
      "The Mean squared error is  0.016801069091246075\n",
      "The adjusted R2 score is  0.8541725633097772\n"
     ]
    }
   ],
   "source": [
    "random_grid = {\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "model = GradientBoostingRegressor()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_data,y_train)\n",
    "best_random_grid=rf_randomcv.best_estimator_\n",
    "y_pred=best_random_grid.predict(X_test_data)\n",
    "# Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the adjusted R²\n",
    "\n",
    "n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "p = X_test_data.shape[1]  # Number of features in the model\n",
    "adjusted_r2_linear = 1 - (1 - r2_linear) * (n - 1) / (n - p - 1)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(\"The R2 score is \",r2_linear)\n",
    "print(\"The Mean squared error is \",mse_linear)\n",
    "print(\"The adjusted R2 score is \",adjusted_r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.6, 'n_estimators': 256, 'learning_rate': 0.05}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 24 is smaller than n_iter=50. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "The R2 score is  0.8712127208709717\n",
      "The Mean squared error is  0.017635147817695408\n",
      "The adjusted R2 score is  0.8469331518548434\n"
     ]
    }
   ],
   "source": [
    "random_grid = {\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "              }\n",
    "model = XGBRegressor()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=model,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_data,y_train)\n",
    "best_random_grid=rf_randomcv.best_estimator_\n",
    "y_pred=best_random_grid.predict(X_test_data)\n",
    "# Evaluate the model performance (e.g., R-squared, Mean Squared Error)\n",
    "\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the adjusted R²\n",
    "\n",
    "n = X_test_data.shape[0]  # Number of observations (samples) in the testing set\n",
    "p = X_test_data.shape[1]  # Number of features in the model\n",
    "adjusted_r2_linear = 1 - (1 - r2_linear) * (n - 1) / (n - p - 1)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(\"The R2 score is \",r2_linear)\n",
    "print(\"The Mean squared error is \",mse_linear)\n",
    "print(\"The adjusted R2 score is \",adjusted_r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 128, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
